CAPITULO 1 1 EXPLORANDO LA INTELIGENCIA ARTIFICIAL A la pregunta ¿qué es la inteligencia? la respuesta constituye que es la capacidad de establecer relaciones, las cuales se manifiestan en los seres humanos a través del pensamiento y la parte intelectual, y en los animales de manera puramente sensorial por medio de los sentidos (Artasanchez & Joshi, 2020). Los humanos tenemos múltiples aspectos que son parte de la inteligencia, como la capacidad de comunicación, aprendizaje, razonamiento abstracto y resolución de problemas, mientras en los animales la inteligencia está relacionado a poderse adaptar al medio ambiente, manifestándose como la capacidad que poseen de la percepción, memoria y toma de decisiones. Entre los humanos y los animales compartimos rasgos básicos de inteligencia como la conciencia del entorno y la habilidad para responder a estímulos (McCarthy, 1956). La reflexión simbólica y de autorreflexión nos permite trascender el mundo tangible y sensorial, crear cultura y tecnología, ganar experiencia y entender conceptualmente el mundo que nos rodea de forma objetiva. La aplicación del conocimiento es útil en el análisis y resolución de problemas, en el razonamiento durante la toma de decisiones; fomenta la predisposición al descubrimiento, invención, creatividad e innovación. La inteligencia permite a las personas afrontar situaciones complejas de forma oportuna, rápida y razonada, y tener la capacidad de predecir el desarrollo y los cambios en el entorno que nos rodea. El año 1950 marca el inicio formal de la inteligencia artificial, debido a que fue entonces cuando se buscó dotar a las máquinas de recursos para resolver problemas de manera autónoma, sin depender del apoyo humano. Si bien la noción de máquina ha estado presente durante muchos años, fue Alan Turing quien sentó las bases de la inteligencia artificial a finales de la Segunda Guerra Mundial, en particular al estudiar científicamente las máquinas inteligentes. Turing había estado trabajando en la teoría de la computabilidad desde la década de 1930 y desarrolló una de las primeras computadoras electromecánicas, se le recuerda por haber desarrollado una máquina para descifrar códigos llamada "bombe" en Bletchley Park, Reino Unido, que descifraba mensajes encriptados con la máquina alemana Enigma. En octubre de 1950, mientras trabajaba en la Universidad de Manchester publicó su artículo "Computing Machinery and Intelligence" en la revista Mind (Oxford University Press), donde 1
introdujo la prueba de Turing, el aprendizaje autónomo, los algoritmos genéticos y el aprendizaje por refuerzo, sentando las bases conceptuales para que los científicos comenzaran a explorar cómo crear programas capaces de resolver problemas ellos mismos, sin la intervención humana. Este artículo es considerado el punto de partida que impulsó la investigación formal en inteligencia artificial, dando origen a una nueva disciplina dedicada a replicar la inteligencia humana en las máquinas. El legado de Turing ha inspirado décadas de avances en aprendizaje automático, procesamiento de lenguaje natural y otros campos clave de la IA. En 1956, el informático John McCarthy (1927-2011) acuñó el término "Inteligencia Artificial" y organizó el influyente proyecto de investigación Dartmouth Summer Research Project on Artificial Intelligence, en el que participaron varios científicos estadounidenses de matemáticas, psicología, ciencias de la computación y teoría de la información como Marvin Minsky, Allen Newell, Arthur Figura 1.1 John Samuel y Herbert Simon. En aquella época, las McCarthy computadoras se utilizaban para tareas aritméticas con Fuente: Wikipedia herramientas de programación muy primitivas. El entusiasmo y esfuerzo pionero de McCarthy y sus colegas en Dartmouth sentaron las bases conceptuales y metodológicas para el nacimiento de la IA como campo formal. Su trabajo en esas primitivas condiciones informáticas es digno de reconocimiento por impulsar una disciplina que décadas después tendría aplicaciones transformadoras. McCarthy se refiere a esa época como la era de "¡Mira mamá, ahora sin manos!". Desde sus inicios, la Inteligencia Artificial (IA) ha tenido como objetivo replicar y ampliar las capacidades humanas mediante la imitación y expansión de la inteligencia humana mediante medios y técnicas artificiales. Para lograr este propósito, la IA se ha centrado en el desarrollo de modelos computacionales que abordan el comportamiento inteligente. Su enfoque se centra en la creación de sistemas informáticos capaces de realizar actividades como la percepción, el razonamiento, el aprendizaje, la asociación, la toma de decisiones y la resolución de problemas complejos. El progreso que se ha alcanzado en la actualidad en IA ha permitido automatizar actividades que antes solo los expertos humanos podían realizar debido a su complejidad 2
o necesidad del juicio de un experto, la IA ha redefiniendo lo que entendemos por procesos cognitivos e inteligentes, hiendo más allá de las restricciones inherentes a la biología humana (Whitby, 2009). La Inteligencia Artificial representa una disciplina dentro de la informática que busca construir máquinas capaces de funcionar de manera autónoma en entornos complejos y en constante cambio, creando sistemas que sean capaces de adaptarse y responder de manera efectiva a situaciones desafiantes. 1.1 Breve historia cronológica de la inteligencia artificial En 1943, Warren McCulloch y Walter Pitts presentaron el primer modelo de red neuronal artificial capaz de aprender y resolver funciones lógicas. Durante la década de 1950, la investigación en inteligencia artificial se enfocó principalmente en los juegos. En 1951, Minsky y Edmonds construyeron el primer computador basado en una red neuronal llamado SNARC. En 1956, Arthur Samuel desarrolló el primer programa heurístico de juego con capacidad de aprendizaje. Ese mismo año, Allen Newell, Herbert Simon y Cliff Shaw inventaron el programa heurístico llamado Logic Theorist, que resolvió correctamente 38 de los primeros 52 teoremas del Principia Mathematica. Este trabajo marcó el inicio de la investigación en psicología cognitiva utilizando ordenadores. En 1957, Noam Chomsky presentó su obra "Estructuras Sintácticas", destacando la importancia de la sintaxis en la investigación del lenguaje formal (Chomsky, 2004). En 1958, John McCarthy inventó el lenguaje Lisp, una herramienta utilizada para la investigación en inteligencia artificial que podía procesar tanto valores numéricos como símbolos (McCarthy, 1960). McCarthy también fue pionero en el desarrollo del concepto de tiempo compartido (Barski, 2010). A principios de la década de 1960, la investigación en inteligencia artificial se centró principalmente en algoritmos de búsqueda y resolución de problemas generales. Allen Newell, Herbert Simon y Cliff Shaw desarrollaron un programa llamado GPS (General Problem Solver), que era un sistema definido por objetos y operadores que se aplicaban a dichos objetos. GPS pudo resolver problemas como las Torres de Hanoi. El sistema utilizaba reglas heurísticas para aprender a partir de sus propios descubrimientos y seguía un enfoque similar al de los humanos para resolver problemas, estableciendo un plan, aplicando axiomas y reglas, y analizando medios y fines para modificar la resolución del problema hasta alcanzar el objetivo. 3
En 1961, Marvin Minsky publicó el artículo "Pasos hacia la Inteligencia Artificial", estableciendo una terminología unificada para la investigación en inteligencia artificial. En 1962, Frank Rosenblatt desarrolló el Perceptrón, una red neuronal utilizada para el reconocimiento visual de patrones, aunque tuvo un impacto limitado en ese momento. En 1965, Edward Feigenbaum, Bruce Buchanan, Joshua Lederberg y Carl Djerassi desarrollaron el primer sistema experto llamado DENDRAL, utilizado para el análisis químico. Ese mismo año, Alan Robinson propuso el principio de la Resolución. En 1968, Ross Quillian introdujo la Red Semántica como una representación del conocimiento. En 1969, se fundó la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), que comenzó a celebrarse cada dos años y, a partir de 2016, se lleva a cabo anualmente. La revista Intelligence Artificial se publica bajo los auspicios de la IJCAI desde 1970. Después de la década de 1970, cuando las redes neuronales no cumplieron sus promesas, época conocida como el "AI hype", la financiación y las actividades de investigación se redujeron drásticamente. Esto se conocio como el "invierno de la IA". A principios de la década de 1970, el campo de la inteligencia artificial se enfocó hacia el estudio y la comprensión del lenguaje natural, así como a la representación avanzada del conocimiento. Durante este período, se observaron progresos significativos en diversas ramas de la IA, marcando un hito en su desarrollo. En 1972, un avance fundamental fue la publicación de Terry Winograd sobre el programa SHRDLU. Este programa se distinguió por su habilidad pionera en comprender el lenguaje natural, específicamente el inglés, demostrando una comprensión profunda y contextual de las interacciones lingüísticas. SHRDLU no solo representó un logro técnico importante, sino que también estableció un marco conceptual crítico para las investigaciones futuras en la comprensión del lenguaje dentro del campo de la inteligencia artificial. Alain Colmerauer, en la Universidad de Marsella en Francia, desarrolló el lenguaje de programación Prolog en el mismo año. Prolog se convirtió en un lenguaje ampliamente utilizado en la programación de IA debido a su capacidad para trabajar con la lógica y el razonamiento (Colmerauer & Roussel, 1996). En 1973, Roger Schank propuso la teoría de la dependencia conceptual, que se centraba en la comprensión del lenguaje natural basada en el conocimiento y la 4
representación semántica. Esta teoría contribuyó al avance en el procesamiento del lenguaje natural en la IA. Marvin Minsky publicó en 1974 la teoría del sistema de marco (A Framework for Representing Knowledge), que fue una contribución importante en la representación del conocimiento en la IA (Baader, 2003). Esta teoría posibilitó que las máquinas ordenaran y estructuraran la información de una forma parecida a como lo hacen los seres humanos, lo que simplificó el proceso de entendimiento y razonamiento (Minsky & others, 1974). En ese mismo año, Edward Shortliffe desarrolló el sistema experto MYCIN como parte de su tesis doctoral, implementado en Lisp, era capaz de diagnosticar trastornos en la sangre y prescribir medicación correspondiente. Este sistema experto fue un hito en el campo médico y demostró el potencial de la IA en el diagnóstico y tratamiento de enfermedades (Shortliffe, 1977). Durante esta época, también se desarrollaron sistemas expertos más avanzados, como el EURISKO, que tenía la capacidad de mejorar automáticamente su conjunto de reglas heurísticas mediante la inducción. Además, en los años 70 se lograron otros avances notables, como la popularización de los algoritmos genéticos gracias al trabajo de John Holland, y la resurrección de las redes neuronales con la creación del algoritmo de retropropagación (backpropagation) por parte de Paul John Werbos. Este algoritmo sigue siendo ampliamente utilizado en el aprendizaje supervisado para el entrenamiento de redes neuronales (Pham & Karaboga, 2012). Asimismo, en esta década se construyó el primer robot capaz de comprender el inglés y se desarrolló el primer vehículo autónomo, marcando importantes avances en la interacción entre la IA y el mundo físico. En 1977, Edward Feigenbaum publicó un paper titulado "El arte de la inteligencia artificial: Temas y estudios de caso en ingeniería del conocimiento" en el quinto IJCAI. En esta publicación dice que la ingeniería del conocimiento es el arte de aplicar los principios y herramientas de la investigación en IA para abordar problemas de aplicaciones difíciles que requieren conocimientos especializados para su solución (Brachman & Levesque, 2004). En 1978, John P. McDermott introdujo un hito en el campo de la inteligencia artificial con el desarrollo del sistema experto R1, marcando uno de los primeros éxitos significativos en esta área. Compuesto por aproximadamente 2500 reglas, R1 fue implementado por DEC (Digital Equipment Corporation) para optimizar los procesos de 5
pedidos de clientes, facilitando la selección de componentes adecuados para sistemas informáticos. Este logro no solo demostró la utilidad práctica de los sistemas expertos, sino que también abrió camino para su aplicación en el mundo empresarial. Posteriormente, a mediados de la década de 1980, se produjeron avances importantes en el algoritmo de aprendizaje por retroalimentación, originalmente desarrollado por Bryson y Ho en 1969. Estas mejoras fueron importantes para el progreso de los métodos de aprendizaje automático, potenciando significativamente el rendimiento y la eficiencia en el entrenamiento de sistemas basados en IA, lo que representó un avance clave en la evolución de las técnicas de inteligencia artificial. Durante los años 80, la Inteligencia Artificial se consolida como una ciencia en sí misma. Judea Pearl en 1982 promueve la idea de los sistemas expertos normativos que se basan en normas y principios de racionalidad formal, los cuales actúan racionalmente de acuerdo con las leyes de la teoría de la decisión, sin intentar replicar directamente el proceso de razonamiento humano. En 1986, Eric Horvitz y David Heckerman que han realizado contribuciones significativas en áreas como el aprendizaje automático, la toma de decisiones basada en datos y la bioinformática, respaldaron la idea de Pearl. En ese mismo año, Rumelhart y McClelland publicaron aplicaciones de algoritmos de aprendizaje en informática y psicología, lo que dio lugar al establecimiento del enfoque del conexionismo basado en redes neuronales. Este enfoque revivió el interés en las redes neuronales como una herramienta poderosa para el aprendizaje automático y estableció una nueva dirección en la investigación y desarrollo de IA. En 1988, Judea Pearl publica el influyente texto "Probabilistic Reasoning in Intelligent Systems", que incorpora conceptos de probabilidades y teoría de decisiones en el campo de la inteligencia artificial. Este trabajo fue fundamental para avanzar en el uso de modelos probabilísticos en la toma de decisiones en sistemas inteligentes. En 1995, Russell y Norvig retomaron el trabajo en agentes totales, sistemas que buscan encontrar soluciones a partir de información de entrada continua. En 1997, la computadora Deep Blue de IBM venció al ajedrecista ruso Garry Kasparov considerado uno de los mejores jugadores de ajedrez, este hecho situó a la inteligencia artificial como protagonista en el ámbito tecnológico y demostró el potencial que tiene al desafiar a expertos humanos en juegos complejos. En el año 2000, se construyó el robot ASIMO, que era capaz de desplazarse en 6
dos pies, dar la mano y contestar a preguntas simples. En 2002, la empresa Cognitec desarrolla el primer sistema comercial de reconocimiento facial a partir de imágenes o videos. En 2006, se presentó el prototipo del robot programable y autónomo NAO. En 2009, se lograron desarrollar robots sociales que son capaces de detectar emociones e interactuar con niños autistas. A partir del 2010 hasta la actualidad, se ha desarrollado el aprendizaje profundo (DL Deep Learning), que es un tipo especial de red neuronal que tiene más de una capa oculta. Esto solo es posible con el aumento de la potencia informática, especialmente de las unidades de procesamiento gráfico (GPU) y algoritmos mejorados. Hasta ahora, el DL ha superado muchos otros algoritmos en un gran conjunto de datos. En 2014, se produjo un acontecimiento importante en el campo de la inteligencia artificial cuando el chatbot Eugene Goostman logró superar el test de Turing, considerado un indicador crítico en la evaluación de la capacidad de las máquinas para imitar la inteligencia humana en el diálogo. Este logro no solo demostró avances significativos en el desarrollo de sistemas de conversación automatizados, sino que también abrió un debate sobre la complejidad y la verosimilitud de las interacciones entre humanos y máquinas, marcando un punto de referencia en la historia de la inteligencia artificial. En el año 2016, AlphaGo, un programa de IA creado por DeepMind Technologies, alcanzó notoriedad mundial al vencer a Lee Sedol, un campeón global en Go, un milenario y desafiante juego de estrategia chino caracterizado por su alta complejidad y las numerosas posibilidades que ofrece cada partida. Este logro marcó un punto de inflexión, generando un renovado interés y avances significativos en el ámbito de la inteligencia artificial, evidenciando así su capacidad para abordar tareas de gran complejidad y para la toma de decisiones estratégicas. Un hito importante en la intersección de la inteligencia artificial y la biología se alcanzó en 2020 gracias a DeepMind, los creadores de AlphaGo. Desarrollaron AlphaFold, una versión mejorada de su sistema de IA, que logró descifrar un desafío que había perplejado a los biólogos por más de cinco décadas: el plegamiento de proteínas. Este fenómeno, en el que las proteínas obtienen su forma tridimensional específica, es importante para comprender su función dentro del organismo. Este avance es fundamental, debido a que desentraña cómo las células operan y cómo se manifiestan diversas enfermedades a nivel molecular, debido a la extraordinaria complejidad de este 7
proceso. A partir del 2021, los avances en inteligencia artificial han sido impresionantes, transformando diversos sectores de manera significativa. Un claro ejemplo es el modelo GPT de OpenAI, que ha marcado un antes y un después en la forma en que las máquinas entienden y generan lenguaje, siendo útil en una variedad de aplicaciones que incluyen desde la generación automatizada de contenido hasta soporte en servicios al cliente. Por otro lado, los modelos como BERT, desarrollados por Google, han revolucionado la manera en que se procesa el lenguaje en búsquedas en línea y otras aplicaciones, logrando una mayor precisión y relevancia en las respuestas brindadas. IBM Watson Health ha transformado el sector médico con su capacidad para analizar extensos volúmenes de datos médicos, contribuyendo a diagnósticos más acertados y tratamientos personalizados. La empresa Boston Dynamics ha sobresalido con sus robots Atlas y Spot, que se han convertido en herramientas importantes para tareas de inspección y rescate, así como en aplicaciones industriales, demostrando la evolución y adaptabilidad de la robótica. Hugging Face, ha permitido la accesibilidad al aprendizaje profundo y procesamiento del lenguaje natural a un público más amplio. En el terreno creativo, proyectos como AIVA (Artificial Intelligence Virtual Artist) están trabajando en la composición musical mediante la inteligencia artificial y DeepArt explora nuevas posibilidades en el arte visual a través de la tecnología de IA. Google AI for Earth está utilizando la inteligencia artificial para el análisis de datos medioambientales, en la búsqueda de soluciones sostenibles, que aborden los retos más críticos del cambio climático. El campo de la inteligencia artificial se suele confundir con la ciencia de datos, el big data y la minería de datos, pero en realidad representan disciplinas diferentes. La ciencia de datos se enfoca principalmente en la manipulación y el análisis de datos, incluyendo el manejo de grandes volúmenes de datos (big data) y la minería de datos; de forma frecuente emplea técnicas de aprendizaje automático (Machine Learning) y aprendizaje profundo (Deep Learing) para el procesamiento de datos. La transformación digital ha generado una inmensa cantidad de datos, los cuales provienen de diversas fuentes, al no poder ser analizados de manera eficiente con las herramientas y algoritmos tradicionales de bases de datos; la inteligencia artificial, el big data y la minería de datos han desarrollo algoritmos sofisticados ( algoritmos de 8
aprendizaje automático, algoritmos de clasificación y regresión, algoritmos de agrupamiento y algoritmos de minería de datos), que posibilitan a las empresas tomar decisiones basadas en datos y brindar a los usuarios acceso a información relevante de manera rápida y cómoda en sus actividades diarias. Google Search y Google Maps proporcionan resultados de búsqueda relevantes y sugerencias de navegación personalizadas comparado con productos similares, Spotify recomienda música que se ajuste a los gustos y preferencias del usuario y Netflix emplea la IA para sugerir películas y series que probablemente interesen al usuario, basándose en sus visualizaciones anteriores y preferencias. Todas estas plataformas populares incorporan algoritmos avanzados de inteligencia artificial para realizar su trabajo. Al usar uno de los siguientes asistentes personales, Alexa de Amazon, Siri de iPhone, Cortana de Microsoft, Bixby de Samsung o Google Assistant, observamos que entiende lo que hablamos, ejecutando de forma eficiente la tarea solicitada. Las redes sociales como Facebook, Pinterest y Google Fotos, tienen incorporado el reconocimiento facial, a través de técnicas sofisticadas de visión por computadora que mejora la interactividad y seguridad de sus usuarios. En el ámbito empresarial, la robótica, los sistemas expertos y el aprendizaje automático están transformando los procesos y brindando oportunidades de innovación en los negocios. Actualmente, se están desarrollando herramientas y plataformas de ciencia de datos que permiten la aplicación de algoritmos de inteligencia artificial. Algunos ejemplos de estas plataformas incluyen RapidMiner, Anaconda y Python. Los líderes de la industria tecnológica, como Google, Microsoft, Meta Platforms, Amazon, Oracle y OpenAI están profundamente comprometidos con la inteligencia artificial. Esta tecnología se está integrando profundamente en diversos aspectos de nuestra vida cotidiana, influyendo en nuestras decisiones y siendo importante en las estrategias empresariales, la salud pública, la seguridad y las finanzas. En el futuro debemos esperar un desarrollo significativo en aplicaciones web, videojuegos y robótica autónoma, veremos cada vez más vehículos autónomos circulando por las calles, robots sociales ayudando a las personas en su vida diaria y exploradores planetarios llegando a lugares más lejanos en el universo. Asimismo, las aplicaciones en el ámbito medioambiental y el ahorro energético serán de gran importancia, al igual que en los 9
campos de la economía, la sociología y el arte (Lee & Qiufan, 2021). 1.2 Contribuciones de las ciencias a la inteligencia artificial Varias ciencias han aportado al desarrollo de la Inteligencia Artificial, entre ellas se tiene: La filosofía ha planteado diversas teorías sobre el razonamiento y el aprendizaje, concibiendo a la mente como una máquina que funciona a partir del conocimiento, codificado en un lenguaje interno y al considerar que el pensamiento sirve para determinar cuál es la acción correcta que se debe emprender (McCarthy, 1990). Un parte relevante en el desarrollo del razonamiento deductivo es la lógica silogística propuesta por el filósofo Aristóteles, considerada el primer sistema formal de razonamiento deductivo, la cual establece reglas precisas para derivar conclusiones válidas a partir de premisas, esto a contribuido al desarrollo posterior de la lógica y ha influido en los fundamentos de la inteligencia artificial. Desde la perspectiva de la inteligencia artificial, se han explorado puntos de vista más avanzados y sofisticados para comprender el razonamiento y el aprendizaje; que involucran el uso de algoritmos de aprendizaje automático, redes neuronales y técnicas de procesamiento de lenguaje natural, permitiendo a las máquinas no solo realizar razonamiento deductivo, sino también abordar problemas complejos de manera más eficiente y aproximarse a la toma de decisiones basadas en datos y modelos de conocimiento. Las matemáticas son fundamentales en el desarrollo de teorías formales relacionadas con la lógica, las probabilidades, la teoría de decisiones y la computación. La lógica matemática permite tratar con declaraciones y razonamientos que pueden ser verdaderos o falsos, mientras que la teoría de probabilidades aborda situaciones de incertidumbre y riesgo. En el libro "Una investigación de las leyes del pensamiento" de George Boole se encuentra el desarrollo del álgebra booleana que se ha convertido en una herramienta esencial en la lógica digital y la computación para la representación de reglas básicas de razonamiento en actividades mentales, permitiendo manipular datos y realizar operaciones lógicas en las computadoras y otros dispositivos electrónicos. Las matemáticas siguen siendo esenciales en el ámbito de la inteligencia artificial. 10
Las redes neuronales y los algoritmos de aprendizaje automático son modelos matemáticos que permiten a las máquinas aprender de los datos y realizar inferencias complejas para tomar decisiones o realizar acciones basadas en la información procesada. La teoría de la computación proporciona el marco teórico y los principios que guían la creación y el funcionamiento de algoritmos que minimicen el uso de recursos, tiempo de computación y memoria; mientras que los fundamentos matemáticos aportan las herramientas y técnicas necesarias para formular y resolver problemas de manera lógica y precisa (Domkin, 2021). La psicología tiene gran importancia en el estudio y entendimiento de la mente humana, al ser una ciencia, se enfoca en explorar y comprender los procesos cognitivos (como el pensamiento, la memoria, la percepción y la toma de decisiones) y emocionales (los sentimientos y las emociones) que caracterizan el comportamiento humano. La idea de que los humanos procesan información ha contribuido a modelar los sistemas de inteligencia artificial, inspirándose en la forma en que los humanos pensamos y tomamos decisiones. Los enfoques teóricos y conceptuales de la psicología sobre el procesamiento del lenguaje natural, la percepción visual y la toma de decisiones, han influido en el diseño de algoritmos y modelos utilizados en los sistemas de la inteligencia artificial. La IA no se centra exclusivamente en imitar la mente humana. También incorpora elementos de matemáticas, estadística e informática, lo que permite a las máquinas realizar tareas cognitivas específicas y tomar decisiones en diversos contextos, a veces de manera diferente a como lo haría un ser humano. La lingüística es importante en la inteligencia artificial al proporcionar teorías y herramientas para comprender la estructura y el significado del lenguaje humano. El lenguaje es un componente fundamental de la comunicación y su comprensión y generación son elementos esenciales en muchos sistemas de inteligencia artificial. La lingüística es la ciencia del lenguaje que posee un enfoque sistemático para el análisis de la gramática (estructura y reglas del lenguaje), semántica (significado de las palabras y frases) y pragmática (uso del lenguaje en contextos específicos) del lenguaje. El estudio detallado de las reglas y patrones lingüísticos, ha permitido los avances tecnológicos en el campo de la inteligencia artificial y la computación, permitiendo a las máquinas entender y generar lenguaje, así como para producir texto comprensible para 11
los humanos. La colaboración entre la lingüística y la inteligencia artificial ha sido un factor importante para el progreso del procesamiento del lenguaje natural (PLN), que permite a las máquinas entender, interpretar y generar lenguaje humano de forma similar a cómo lo hacen las personas. Otras aplicaciones son la traducción automática; los chatbots, que pueden interactuar en lenguaje natural con los usuarios; y los sistemas de diálogo, que facilitan una comunicación fluida y natural con las máquinas. La lingüística ha sido fundamental en la creación de recursos como bases de datos léxicas (listas de palabras y sus significados), corpus textuales (grandes colecciones de textos escritos o transcripciones de habla) y modelos de conocimiento lingüístico. Estos recursos son indispensables para entrenar y mejorar los sistemas de IA, proporcionando la información necesaria para que las máquinas aprendan y entiendan el lenguaje humano. La ciencia de la computación proporciona los lenguajes de programación que son el medio por el cual se escriben y se ejecutan los algoritmos de inteligencia artificial y los marcos de desarrollo, que ofrecen estructuras y librerías predefinidas para facilitar la creación de aplicaciones de inteligencia artificial. Alan Turing es una figura fundamental en el origen de la ciencia de la computación debido a sus contribuciones innovadoras y su visión de lo que podrían ser las computadoras y su funcionamiento. Sus teorías y modelos proporcionan el marco conceptual para comprender cómo las máquinas pueden no solo procesar información, sino también realizar tareas que requieren algún nivel de inteligencia, como el aprendizaje, el razonamiento y la adaptación. En el ámbito de la IA, los programas y algoritmos son fundamentales para capacitar a las máquinas para que realicen tareas consideradas inteligentes como la resolución de problemas hasta la imitación de comportamientos humanos como el aprendizaje y la toma de decisiones. Los programas demandan una cantidad significativa de recursos computacionales, incluyendo un alto poder de procesamiento y una gran capacidad de memoria. La eficiencia de un algoritmo se refiere a cuán bien utiliza los recursos disponibles (como tiempo de cómputo y memoria) para lograr sus objetivos La ciencia de la computación ha avanzado en el procesamiento de datos (capacidad de manejar y transformar datos en información útil), el aprendizaje automático (sistemas capaces de aprender y mejorar a partir de la experiencia), la optimización de 12
algoritmos (velocidad y uso de recursos) y la gestión de grandes volúmenes de información (procesar y analizar grandes conjuntos de datos). La neurociencia se centra en entender el funcionamiento del cerebro humano a niveles: molecular, celular y de comportamiento. Este estudio permite desarrollar modelos y principios que son aplicados en la creación de inteligencia artificial. También, proporciona información valiosa sobre cómo se procesa el lenguaje en el cerebro humano, que es utilizado en el procesamiento de lenguaje natural dentro de la inteligencia artificial. La neurociencia ha permitido obtener un conocimiento más profundo de la funcionalidad cerebral y de los procesos de procesamiento de información dentro del cerebro. Este conocimiento es valioso para los expertos en inteligencia artificial, debido a que les ofrece inspiración basada en los mecanismos y procesos cerebrales reales para desarrollar algoritmos y modelos en el campo del aprendizaje automático. La neurociencia ha revelado principios fundamentales sobre cómo funcionan las redes neuronales en el cerebro y cómo este tiene la capacidad de cambiar y adaptarse a través del aprendizaje (plasticidad cerebral). Estos descubrimientos son importantes para el diseño de algoritmos en el aprendizaje automático, especialmente en el desarrollo de redes neuronales artificiales. Estos algoritmos buscan imitar los principios biológicos del cerebro humano para mejorar la eficiencia y capacidad de procesamiento en la inteligencia artificial. La ciencia cognitiva es decisiva en el estudio de las actividades mentales humanas, como la percepción, el aprendizaje, la memoria, el pensamiento y la conciencia. Esta disciplina integra conocimientos de varias áreas de estudio como la psicología que contribuye con la comprensión del comportamiento y los procesos mentales; la lingüística que aporta conocimientos sobre el lenguaje y su relación con el pensamiento; la filosofía que ofrece marcos conceptuales y éticos; la neurociencia que brinda información sobre la estructura y función del cerebro; y la inteligencia artificial que ayuda a modelar y simular procesos cognitivos. Juntas, estas disciplinas permiten una comprensión más completa de cómo se desarrollan y trabajan las actividades mentales en los seres humanos. La investigación en ciencia cognitiva busca descubrir cómo funcionan internamente los procesos a través de los cuales los seres humanos reciben, procesan y comprenden la información que obtienen de su entorno para desarrollar máquinas que puedan procesar e interpretar información sensorial (como imágenes, sonidos, texturas) 13
de manera similar a cómo lo hace un ser humano. La investigación en ciencia cognitiva también ha explorado el pensamiento humano, incluyendo la resolución de problemas, la toma de decisiones y el razonamiento. Estos procesos cognitivos han sido estudiados y modelados para desarrollar algoritmos y sistemas de inteligencia artificial capaces de realizar tareas complejas de manera lógica y eficiente. Los descubrimientos, que detallan los procesos mediante los cuales los humanos adquieren conocimientos (aprendizaje), los retienen (memoria) y los recuperan cuando es necesario, proporcionan una base sólida para la creación de modelos sofisticados en el campo del aprendizaje automático. Se ha explorado el pensamiento humano en la búsqueda de entender cómo los humanos abordan y solucionan situaciones complejas, y cómo procesan información para llegar a conclusiones con el objetivo de crear máquinas con inteligencia artificial que imiten el comportamiento humano. Comprender qué es la conciencia y cómo funciona sigue siendo un desafío en búsqueda de entender en profundidad la mente humana, a pesar de estos esfuerzos que se han hecho en la inteligencia artificial, simular completamente la conciencia humana es un desafío significativo y aún no se ha logrado. Los modelos actuales pueden aproximarse a ciertos aspectos de la conciencia, pero la experiencia subjetiva y la autoconciencia plena son elementos particularmente difíciles de replicar en una máquina (McDermott, 2007). 1.3 Areas de investigación de la inteligencia artificial La inteligencia artificial (IA) representa un cambio significativo y un avance considerable en el campo de la tecnología informática, muestra el progreso que ha tenido en comparación con generaciones anteriores, que se centraban principalmente en realizar cálculos numéricos (como los usados en aplicaciones científicas o comerciales), actualmente la IA se orienta hacia tareas más complejas y abstractas, que incluyen la manipulación simbólica (trabajar con información representada mediante símbolos, como en el procesamiento del lenguaje) y la emulación de comportamientos inteligentes (intentar replicar o simular formas de inteligencia similares a las humanas, como el reconocimiento de patrones, la toma de decisiones, y el aprendizaje). Las áreas de la IA en las cuales se está investigando en la actualidad son presentadas en la figura 1.2, se han agrupado partiendo de las formas de representación 14
del conocimiento en la inteligencia artificial que son: la representación simbólica y la representación no simbólica y también se puede observar combinadas de las dos representaciones. La representación simbólica es un método para modelar y manipular el conocimiento utilizando símbolos (palabras, números o iconos) que se eligen y se utilizan de manera estructurada y lógica. Por ejemplo, en un sistema simbólico, una palabra como "elefante" = “mamífero paquidermo de gran tamaño” es un símbolo que representa un concepto específico en el mundo real. La representación no simbólica, en lugar de usar reglas específicas y símbolos definidos, utiliza patrones y modelos matemáticos o estadísticos que aprenden de los ejemplos y datos. Por ejemplo, en una red neuronal utilizada para el reconocimiento de imágenes, no hay un símbolo específico para "elefante", sino que el conocimiento sobre qué constituyen estos animales se aprende (de muchas fotos de elefantes) y se almacena en los patrones de conexión entre las neuronas de la red. Figura 1.2 Áreas de investigación de la inteligencia artificial Elaborado por los Autores Se presenta una breve explicación de cada una de las áreas den investigación en inteligencia artificial: Representación Simbólica: 1. Sistemas Expertos: Utilizan reglas para representar el conocimiento y la lógica para deducir nuevas informaciones. 15
2. Procesamiento del Lenguaje Natural (PLN): Se enfoca en la comprensión y generación de lenguaje humano mediante reglas gramaticales y semánticas. 3. Robótica Cognitiva: Los robots son capaces de interpretar su entorno y reconocer los objetos y situaciones que encuentran, tienen la capacidad de interactuar físicamente con su entorno, tomando o moviendo los objetos (Franceschetti, 2018). 4. Razonamiento Automatizado: Las máquinas o sistemas informáticos son capaces de pensar y llegar a conclusiones de manera similar a como lo haría un ser humano, proceso que lo realizan de manera automática y estructurada mediante un programa o algoritmo. 5. Interacción Hombre-Máquina: Es el estudio y la parte práctica de cómo las personas se comunican o interactúan con las computadoras y otros dispositivos tecnológicos, buscando que sea fluido y sencillo como hablar o interactuar con otro ser humano. 6. Ontologías y Representación del Conocimiento: Se trata de crear modelos que puedan representar el conocimiento de manera que las computadoras lo entiendan y lo manejen eficientemente, utilizando entidades, atributos, y relaciones simbólicas (Poole & Mackworth, 2010). Representación No-Simbólica: 1. Aprendizaje Automático: Se tienen los siguientes tipos de aprendizaje. a) Aprendizaje Supervisado: Estos modelos de inteligencia artificial se utilizan para realizar tareas de clasificación o regresión, se entrenan usando un conjunto de datos que han sido etiquetados con anterioridad (Xiao, 2022). b) Aprendizaje No Supervisado: Los modelos trabajan identificando patrones en datos que no han sido etiquetados, para locual usan técnicas de grupamiento o reducción de dimensionalidad. c) Aprendizaje por Refuerzo: Son modelos (agentes) que aprenden a tomar decisiones mediante experimentación e interacción con el medio ambiente, reciben recompensas o penalizaciones en función de las acciones que realizan. 2. Redes Neuronales y Aprendizaje Profundo: Es un tipo de tecnología que intenta imitar la forma en que el cerebro humano procesa la información. Las redes neuronales son sometidas a un proceso llamado "entrenamiento", durante el cual 16
la red ajusta sus parámetros internos (llamados "pesos sinápticos") para mejorar su capacidad de realizar la tarea deseada (Haykin, 2009). 3. Visión por Computadora: Busca que las computadoras procesen y analicen el contenido visual (imágenes, videos) obtenido mediante cámaras y sensores del mundo que les rodea. 4. Procesamiento de Lenguaje Natural basado en Aprendizaje Automático: Usa modelos estadísticos y redes neuronales para el entendimiento, interpretación y generación de lenguaje humano de manera efectiva. 5. Sistemas de Recomendación: Son sistemas informáticos que se basan en algoritmos de aprendizaje profundo y estan diseñados para sugerir automáticamente productos, servicios, información o acciones a los usuarios. 6. Bioinformática y Análisis de Datos Biomédicos: Utiliza técnicas informáticas y estadísticas para entender y analizar los datos. La bioinformática se centra en el análisis de datos a nivel molecular, como los genes y proteínas, mientras que el análisis de datos biomédicos incluye datos relacionados con la salud y la biología. 7. Análisis Predictivo y Minería de Datos: Son técnicas que buscan patrones (como tendencias comunes o repetitivas), correlaciones (relaciones entre diferentes variables) y conocimientos a partir de grandes volúmenes de datos para predecir tendencias o comportamientos futuros. 8. Integración de Conocimiento en Modelos de Aprendizaje Automático: Se refiere al proceso de aprovechar la información y estructuras que ya existen he incorporárlas en los modelos de aprendizaje automático. 9. Simulación y Modelado Basado en Agentes: Se usa agentes autónomos para simular comportamientos y sistemas complejos y dinámicos. Cada agente en el modelo representa una entidad con su propio conjunto de características y reglas de comportamiento. Esta técnica es útil en el campo de la inteligencia artificial distribuida, donde se busca comprender y replicar comportamientos colectivos o de enjambre, como los observados en la naturaleza. 10. Robótica Basada en Aprendizaje Automático: Incorpora técnicas de aprendizaje automático en la robótica para mejorar las capacidades de percepción y decisión de los robots, especialmente en entornos que son dinámicos y no estructurados (Franceschetti, 2018). 17
Representación Simbólica y Representación No-Simbólica: 1. Procesamiento del Lenguaje Natural (PLN) Híbrido: Estudia cómo hacer que las computadoras sean capaces de analizar, entender y generar lenguaje humano de forma automática. Usa métodos simbólicos como el análisis sintáctico basado en reglas gramaticales y la representación semántica basada en reglas. Combina con métodos no simbólicos, como el aprendizaje profundo. Se emplean en la traducción automática entre idiomas, el análisis de sentimientos y la generación de texto. 2. Sistemas Expertos Mejorados: Los sistemas expertos tradicionales, basados en conocimiento simbólico y reglas, se pueden enriquecer con técnicas de aprendizaje automático para manejar la incertidumbre, aprender patrones y capturen excepciones al manejar grandes volúmenes de datos. 3. Robótica Cognitiva Avanzada: Los robots utilizan algoritmos de aprendizaje profundo para interpretar lo que perciben a través de sus sensores y están equipados con sistemas que siguen reglas predefinidas para planificar sus acciones y tomar decisiones (Franceschetti, 2018). Estos robots pueden manejar una variedad de situaciones y tareas en diferentes entornos, realizando ajustes según sea necesario para adaptarse a cambios inesperados en su entorno. 4. Sistemas de Recomendación Híbridos: Combinan el análisis de patrones de comportamiento de usuarios (aprendizaje no simbólico) con reglas basadas en conocimiento específico del dominio (aprendizaje simbólico) para proporcionar recomendaciones más precisas y contextuales. Son asistentes inteligentes que te sugiere cosas basándose en lo que te gusta y en conocimientos específicos de un área determinada. 5. Integración de Conocimiento en Aprendizaje Automático: Se utiliza en proyectos que buscan integrar bases de conocimiento estructuradas (simbólicas) dentro de modelos de aprendizaje automático (no simbólicos) para mejorar la interpretación y generalización de los modelos. 6. Ontologías en Aprendizaje Automático: Se usa ontologías y estructuras de conocimiento simbólico para guiar y mejorar el proceso de aprendizaje en modelos no simbólicos, especialmente en dominios específicos como la medicina o la biología (García Serrano, 2012). 18
7. Inteligencia Artificial Explicable (XAI): Busca desarrollar modelos de IA, especialmente los basados en aprendizaje profundo, que puedan explicar sus decisiones y procesos de una manera comprensible para los humanos, a menudo integrando elementos simbólicos en la explicación de modelos no simbólicos. 1.4 Influencia significativa de la inteligencia artificial La importancia de la inteligencia artificial radica en el hecho de que se relaciona con la mayoría de campos de investigación a través de diferentes disciplinas tales como: las ciencias de la ingeniería (ayuda a diseñar sistemas más eficientes y seguros), la economía (permite analizar tendencias de mercado y predecir cambios económicos) y la medicina (contribuye al diagnóstico y tratamiento de enfermedades, e incluso en la investigación de nuevos medicamentos), siendo usada la IA como un medio para hacer frente a muy complicados y difíciles procesos de cómputo, así como a los problemas cognitivos. Al ser la IA una de las principales corrientes de tratamiento de la información, puede ahora ofrecer soluciones a los problemas utilizando los avances e innovaciones entre una amplia gama de sub-áreas que inducen el pensamiento y razonamiento en los modelos y sistemas desarrollados. Se espera que las empresas que utilizan aplicaciones de Inteligencia Artificial mejoren la capacidad de analizar los datos a través de múltiples variables, puedan detectar fraudes y gestionen de mejor manera las relaciones con los clientes para obtener una ventaja competitiva. Altos directivos de muchas empresas utilizan sistemas de planificación estratégica basados en IA para tener una asistencia en funciones como: análisis de la competencia, el despliegue de tecnología y la asignación de recursos. También se utilizan programas para ayudar en el diseño de la configuración de equipos, distribución de productos, asesoramiento en el cumplimiento y evaluación de personal. La IA está contribuyendo en gran medida en la organización, planificación de la gestión y control de las operaciones, y continuará haciéndolo con más frecuencia a medida que se refinan los programas. La IA también es influyente en la ciencia y la ingeniería, las aplicaciones desarrolladas se utilizan para organizar y manipular las cantidades cada vez mayores de información disponibles para los científicos e ingenieros, se emplea en clasificaciones biológicas, en la creación de circuitos semiconductores y componentes de automóviles. La IA se utiliza en la difracción y análisis de imágenes, en las plantas de energía y en el 19
diseño de las estaciones espaciales, uno de sus mayores usos es en la robótica. Por la cantidad de habitantes que viven en el planeta tierra cada vez más se necesita usar de forma eficiente los recursos materiales y humanos, para lo cual podemos utilizar el poder que tienen las computadoras y la IA, en la agricultura para controlar plagas y manejar cultivos en forma más eficiente; en las fábricas en la realización de montajes peligrosos y actividades tediosas (labores de inspección y mantenimiento); en medicina en el diagnóstico a pacientes, detectar pacientes que están en mayor riesgo de complicaciones, supervisar la condición de los pacientes, descubrir sutiles interacciones entre los medicamentos que ponen a los pacientes en riesgo de sufrir efectos secundarios graves, administrar tratamientos y preparar estudios estadísticos; en el trabajo doméstico asistir en la limpieza de la casa, preparación de alimentos, a brindar asesoría acerca de dietas, compras, supervisión y gestión de consumo energético y seguridad del hogar; en las escuelas apoyar en la formación de los estudiantes, especialmente en aquellas materias consideradas complejas; colaborar con los humanos expertos en el análisis para la solución de problemas difíciles o en el diseño de nuevos dispositivos. Se ha convertido en algo común de nuestras vidas el uso del GPS (Global Positioning System) para escoger la mejor ruta, recibir estados del tráfico, usar teléfonos inteligentes que entienden nuestro lenguaje, asistentes inteligentes como Cortana de Microsoft y Siri de Apple; la búsqueda inteligente que realiza el buscador de Google y Bing, el traductor y lector de documentos de Google. Algoritmos inteligentes que detectan los rostros mientras estamos tomando una foto con el celular y reconocen las caras de las personas cuando se publican las fotos en Facebook. Los usuarios de Amazon reciben recomendaciones sobre libros, los de Netflix sobre películas. Aceleración de las portátiles al adivinar lo que se hará a continuación. Los coches inteligentes que se conducen a sí mismos, que pueden enfrentar situaciones complicadas, que reconocen la voz para una interacción natural, que están desarrollando BMW, Tesla, Google. La lucha contra el spam (correo basura) a través de clasificadores bayesianos y otras técnicas probabilísticas han mostrado su eficacia. Los militares están usando aviones no tripulados, drones y robots para investigar en lugares peligrosos. En la actualidad existe dispositivos con IA que permiten que la gente ciega vea, los sordos escuchen, y los discapacitados y ancianos puedan caminar, e incluso correr. Las galaxias están siendo exploradas en mutuo apoyo entre los astrónomos 20
experimentados y las máquinas. La presencia de los métodos de Inteligencia artificial ha puesto a los seres humanos a pensar en los riesgos potenciales de sus avances, piensan algunas personas que los sistemas de IA se podrían convertir en superinteligentes y poner en peligro la supervivencia de la humanidad, pero se tiene a la AAAI (Association for the Advancement of Artificial Intelligence) que está encargada de incentivar el avance en la ciencia y tecnología de la inteligencia artificial y promocionar su uso responsable. Los errores de programación en software de inteligencia artificial deben ser considerados, por lo que se requiere de la verificación y validación del software debido a la creciente complejidad y el uso en funciones delicadas como en automóviles, robots quirúrgicos y sistemas de armas. Un desafío técnico es garantizar que los sistemas integrados de forma automática a través de métodos estadísticos se comporten correctamente en el proceso de aprendizaje automático. Otro desafío es garantizar el buen comportamiento cuando un sistema de IA se encuentra con situaciones imprevistas. Los vehículos automatizados, robots para el hogar, y los servicios inteligentes en la nube deben funcionar bien, incluso cuando reciben insumos sorprendentes o confusos (Lalanda et al., 2013). Otro riesgo que está presente son los ciberataques a los cuales los programas de IA son también vulnerables, y si se piensa en lo que podría suceder al estar los algoritmos de IA encargados de tomar decisiones de alto poder. Por lo que se requiere la investigación en seguridad cibernética para estar seguros que los algoritmos de IA y los sistemas que se desarrollan basados en estos pueden sobrevivir los ataques cibernéticos a gran escala. Un aspecto importante de cualquier sistema de IA que interactúa con la gente es que debe razonar sobre lo que las personas piensan en lugar de llevar a cabo los comandos de manera literal, para juzgar si lo que se le pide que realice es normal o razonable para la mayoría de la gente. 1.5 Turing y la Evaluación de la Inteligencia de las Máquinas 21